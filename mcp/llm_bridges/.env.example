# LLM Bridge Configuration
# Copy this file to .env and add your API keys

# ===========================================
# API Keys (at least one required)
# ===========================================
GEMINI_API_KEY=your-gemini-api-key-here
OPENAI_API_KEY=your-openai-api-key-here
XAI_API_KEY=your-grok-api-key-here

# ===========================================
# Default Models (optional)
# ===========================================
# If not specified, uses provider defaults
GEMINI_DEFAULT_MODEL=gemini-2.5-pro
OPENAI_DEFAULT_MODEL=gpt-4o
GROK_DEFAULT_MODEL=grok-2

# ===========================================
# Shared Configuration
# ===========================================
# Maximum file size to process (in bytes)
MAX_FILE_SIZE=26214400  # 25MB

# Maximum total tokens across all files
MAX_TOTAL_TOKENS=120000

# Enable parallel provider queries
ENABLE_PARALLEL=true

# Request timeout in milliseconds
REQUEST_TIMEOUT=120000  # 2 minutes

# ===========================================
# Provider-Specific Settings (optional)
# ===========================================
# Gemini
GEMINI_MAX_TOKENS=1000000
GEMINI_API_VERSION=v1beta

# OpenAI
OPENAI_MAX_TOKENS=128000
OPENAI_ORGANIZATION_ID=

# Grok
GROK_MAX_TOKENS=131072
GROK_API_BASE_URL=https://api.x.ai/v1

# ===========================================
# Feature Flags (optional)
# ===========================================
# Show token counts in responses
SHOW_TOKEN_COUNTS=false

# Show response timing
SHOW_TIMING=true

# Include model info in responses
SHOW_MODEL_INFO=true